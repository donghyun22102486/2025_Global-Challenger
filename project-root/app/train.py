# train.py
import os
import joblib
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import traceback


def train_model(
    df: pd.DataFrame, target_col: str, save_path: str, model_type: str = "rf"
) -> Dict:
    """
    - ì…ë ¥ dfì—ì„œ target_colì„ ë¶„ë¦¬í•˜ê³  ë‚˜ë¨¸ì§€ë¡œ ì›í•« ì¸ì½”ë”© í›„ íšŒê·€ ëª¨ë¸ í•™ìŠµ
    - íƒ€ê¹ƒì´ ì´ì§„ ë¬¸ìì—´ ë“±ì¼ ê²½ìš° ìˆ«ìë¡œ ì•ˆì „ ë³€í™˜
    - êµì°¨ê²€ì¦ ì‹¤íŒ¨í•´ë„ ì €ì¥ì€ ë°˜ë“œì‹œ ìˆ˜í–‰
    - ì €ì¥ í¬ë§·: joblib.dump({"model": model, "features": [..]})
    """

    def _normalize_name(s: str) -> str:
        s = str(s)
        s = s.strip().strip("'").strip('"')
        s = s.replace(" ", "").replace("_", "").lower()
        return s

    def _resolve_target(col_name: str, columns) -> str:
        norm = _normalize_name(col_name)
        # 1) ì™„ì „ ì¼ì¹˜(ì •ê·œí™” ê¸°ì¤€)
        for c in columns:
            if _normalize_name(c) == norm:
                return c
        # 2) ë³µìˆ˜í˜•/s ë³´ì •
        alt = norm + "s" if not norm.endswith("s") else norm[:-1]
        for c in columns:
            if _normalize_name(c) == alt:
                return c
        # 3) ë¶€ë¶„ ì¼ì¹˜(ì•ˆì „í•˜ê²Œ ê¸¸ì´ 4 ì´ìƒì¼ ë•Œë§Œ)
        if len(norm) >= 4:
            for c in columns:
                if norm in _normalize_name(c):
                    return c
        return col_name  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

    if target_col not in df.columns:
        raise ValueError(
            f"âŒ target_col '{target_col}' not found in DataFrame columns."
        )

    # y ì •ë¦¬: object â†’ ìˆ«ì ì‹œë„, ì‹¤íŒ¨ì‹œ ì´ì§„ {0,1} ë§µí•‘ ì‹œë„
    y_raw = df[target_col]
    y = y_raw.copy()

    if y.dtype == "O":
        try:
            y = pd.to_numeric(y, errors="raise")
        except Exception:
            vals = sorted(y_raw.dropna().unique().tolist())
            if len(vals) == 2:
                mapping = {vals[0]: 0, vals[1]: 1}
                y = y_raw.map(mapping)
                print(f"â„¹ï¸ targetì´ ë²”ì£¼í˜•ì´ë¼ {mapping} ë¡œ ë§¤í•‘í–ˆìŠµë‹ˆë‹¤.")
            else:
                raise ValueError(
                    f"âŒ target '{target_col}' is non-numeric with {len(vals)} classes: {vals}"
                )

    # X ë§Œë“¤ê¸°
    X = pd.get_dummies(df.drop(columns=[target_col]), drop_first=False)

    # íŠ¹ì„±/í‘œë³¸ ê²€ì¦
    if X.shape[0] < 5:
        raise ValueError(f"âŒ Too few rows to train: {X.shape[0]}")
    if X.shape[1] == 0:
        raise ValueError("âŒ No feature columns after preprocessing (X has 0 columns).")

    # NaN/Inf ì²˜ë¦¬
    X = X.replace([np.inf, -np.inf], np.nan)
    if X.isna().any().any():
        num_cols = X.select_dtypes(include="number").columns
        X[num_cols] = X[num_cols].fillna(X[num_cols].mean())
        obj_cols = X.select_dtypes(exclude="number").columns
        if len(obj_cols):
            X[obj_cols] = X[obj_cols].fillna("missing")
        print("â„¹ï¸ Xì˜ NaNì„ ê¸°ë³¸ ì „ëµìœ¼ë¡œ ë³´ì •í–ˆìŠµë‹ˆë‹¤.")
    if pd.isna(y).any():
        keep_idx = ~pd.isna(y)
        X, y = X.loc[keep_idx], y.loc[keep_idx]
        print(f"â„¹ï¸ y ê²°ì¸¡ í–‰ ì œê±°: ë‚¨ì€ í–‰ {len(y)}")

    # í•™ìŠµ/í‰ê°€
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    if model_type == "rf":
        model = RandomForestRegressor(random_state=42)
    elif model_type == "gbr":
        model = GradientBoostingRegressor(random_state=42)
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…ì…ë‹ˆë‹¤. 'rf' ë˜ëŠ” 'gbr'ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    try:
        model.fit(X_train, y_train)
    except Exception as e:
        traceback.print_exc()
        raise RuntimeError(f"âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")

    preds = model.predict(X_test)
    rmse = float(np.sqrt(mean_squared_error(y_test, preds)))
    mae = float(mean_absolute_error(y_test, preds))
    r2 = float(r2_score(y_test, preds))

    # êµì°¨ê²€ì¦ì€ ì‹¤íŒ¨í•´ë„ ì €ì¥ ê³„ì†
    try:
        cv_scores = cross_val_score(model, X, y, cv=5, scoring="r2")
        cv_mean = round(float(cv_scores.mean()), 4)
    except Exception as e:
        print(f"âš ï¸ cross_val_score ì‹¤íŒ¨(r2): {e}")
        cv_mean = float("nan")

    # ì €ì¥
    os.makedirs(save_path, exist_ok=True)
    timestamp = datetime.now().strftime("%m%d_%H%M")
    model_bundle = {"model": model, "features": list(X.columns)}
    model_path = os.path.join(save_path, f"{timestamp}.pkl")
    try:
        joblib.dump(model_bundle, model_path)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {os.path.abspath(model_path)}")
    except Exception as e:
        traceback.print_exc()
        raise RuntimeError(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    # feature importance ì €ì¥(ê°€ëŠ¥í•  ë•Œë§Œ)
    importance_path = None
    if hasattr(model, "feature_importances_"):
        importances = dict(zip(X.columns, model.feature_importances_))
        importance_path = os.path.join(
            save_path, f"{timestamp}_feature_importance.json"
        )
        try:
            import json

            with open(importance_path, "w", encoding="utf-8") as f:
                json.dump(importances, f, ensure_ascii=False)
            print(f"ğŸ’¾ ì¤‘ìš”ë„ ì €ì¥ ì™„ë£Œ: {os.path.abspath(importance_path)}")
        except Exception as e:
            print(f"âš ï¸ ì¤‘ìš”ë„ ì €ì¥ ì‹¤íŒ¨: {e}")
            importance_path = None

    return {
        "rmse": round(rmse, 4),
        "mae": round(mae, 4),
        "r2": round(r2, 4),
        "cv_r2": cv_mean,
        "model_path": model_path,
        "importance_path": importance_path,
        "timestamp": timestamp,
    }
